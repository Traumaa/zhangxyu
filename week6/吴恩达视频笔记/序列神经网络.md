# WEEK6：序列神经网络

## 6.1 第一章：循环神经网络

### 6.1.1 序列模型的应用

这些序列模型基本都属于监督式学习，输入 x 和输出 y 不一定都是序列模型。如果都是序列模型的话，模型长度不一定完全一致。

- 语音识别：将输入的语音信号直接输出相应的语音文本信息。无论是语音信号还是文本信息均是序列数据。
- 音乐生成：生成音乐乐谱。只有输出的音乐乐谱是序列数据，输入可以是空或者一个整数。
- 情感分类：将输入的评论句子转换为相应的等级或评分。输入是一个序列，输出则是一个单独的类别。
- DNA序列分析：找到输入的DNA序列的蛋白质表达的子序列。
- 机器翻译：两种不同语言之间的想换转换。输入和输出均为序列数据。
- 视频行为识别：识别输入的视频帧序列中的人物行为。
- 命名实体识别：从输入的句子中识别实体的名字。

![](image/2022-07-27-10-11-11.png)


### 6.1.2 符号定义

**Harry Potter and Hermione Granger invented a new spell.**

对于输入 x，表示为：

$\left[\mathrm{x}^{(1>} \mathrm{x}^{(2>} \mathrm{x}^{(3>} \mathrm{x}^{(4>} \mathrm{x}^{(5>} \mathrm{x}^{(6>} \mathrm{x}^{(7>} \mathrm{x}^{(8>} \mathrm{x}^{(9>}\right]$

$\mathrm{x}^{(t)}$ 表示序列对应位置的输入，$T_x$表示输入序列长度。

该句话包含 9 个单词，输出 y 即为 1 x 9 向量，每位表征对应单词是否为人名的一部分，1 表示是，0 表示否。很明显，该句话中 “Harry”，“Potter”，“Hermione”，“Granger” 均是人名成分，所以，对应的输出 y 可表示为：

$\mathrm{y}=\left[\begin{array}{lllllllll}1 & 1 & 0 & 1 & 1 & 0 & 0 & 0 & 0\end{array}\right]$

$\mathrm{y}^{(t)}$表示序列对应位置的输入，$T_y$表示输入序列长度，则$1 \leq \mathrm{t} \leq \mathrm{T}_{\mathrm{y}}$


$\mathrm{x}^{(t)}$用 one-hot 编码表示，首先定义词库找出单词对应索引，然后用高维词库对应的高维向量表示。

如果出现词汇表之外的单词，可以使用 UNK 或其他字符串来表示。

对于多样本，以上序列模型对应的命名规则可表示为：$\mathrm{X}^{(\mathrm{i})(\mathrm{t})}, \mathrm{y}^{(\mathrm{i})(\mathrm{t})}, \mathrm{T}_{\mathrm{x}}^{(\mathrm{i})}, \mathrm{T}_{\mathrm{y}}^{(\mathrm{i})}$

其中，i 表示第 i 个样本。不同样本的$\mathrm{T}_{\mathrm{x}}^{(\mathrm{i})}$或$\mathrm{T}_{\mathrm{y}}^{(\mathrm{i})}$都有可能不同。


### 6.1.3 循环神经网络模型

**传统标准的神经网络：**

对于序列模型，如果使用标准的神经网络，其模型结构如下：

![](image/2022-07-27-10-17-41.png)

使用标准的神经网络模型存在两个问题：

- 不同样本的输入序列长度或输出序列长度不同，即 $\mathrm{T}_{\mathrm{x}}^{(\mathrm{i})} \neq \mathrm{T}_{\mathrm{x}}^{(\mathrm{j})}, \mathrm{T}_{\mathrm{y}}^{(\mathrm{i})} \neq \mathrm{T}_{\mathrm{y}}^{(\mathrm{j})}$，造成模型难以统一。解决办法之一是**设定一个最大序列长度**，对每个输入和输出序列补零并统一到最大长度。但是**这种做法实际效果并不理想**。
- 第二个问题，这种标准神经网络结构**无法共享序列不同$\mathrm{x}^{(t>}$之间的特征**。

**循环神经网络：**

标准的神经网络不适合解决序列模型问题，而循环神经网络（RNN）是专门用来解决序列模型问题的。RNN 模型结构如下：

![](image/2022-07-29-18-46-52.png)

序列模型从左到右，依次传递，此例中，$T_x = T_y$。$\mathrm{x}^{(t>}$到$\mathrm{y}^{(t>}$之间是隐藏神经元。$\mathrm{a}^{(t>}$会传入到第 t+1 个元素中，作为输入。其中$\mathrm{a}^{(0>}$一般为零向量。

RNN 模型包含三类权重系数，分别是$W_{ax}, W_{aa}, W_{ya}$，**且不同元素之间同一位置共享同一权重系数。**


RNN 的正向传播（Forward Propagation）过程为：

$\mathrm{a}^{(\mathrm{t}>}=\mathrm{g}\left(\mathrm{W}_{\mathrm{aa}} \cdot \mathrm{a}^{(\mathrm{t}-1>}+\mathrm{W}_{\mathrm{ax}} \cdot \mathrm{x}^{(\mathrm{t}>}+\mathrm{ba}\right)$


$\hat{\mathrm{y}}^{(\mathrm{t}>}=\mathrm{g}\left(\mathrm{W}_{\mathrm{ya}} \cdot \mathrm{a}^{(\mathrm{t}>}+\mathrm{b}_{\mathrm{y}}\right)$

其中，g(x) 表示激活函数，不同的问题需要使用不同的激活函数。

### 6.1.4 穿越时间的反向传播

针对上面识别人名的例子，经过 RNN 正向传播，单个元素的 Loss function 为：

$\mathrm{L}^{(\mathrm{t}>}\left(\hat{\mathrm{y}}^{(\mathrm{t}>}, \mathrm{y}^{(\mathrm{t}>}\right)=-\mathrm{y}^{(\mathrm{t}>} \log \hat{\mathrm{y}}^{(\mathrm{t}>}-\left(1-\mathrm{y}^{(\mathrm{t}>}\right) \log \left(1-\hat{\mathrm{y}}^{(\mathrm{t}>}\right)$

该样本所有元素的 Loss function 为：

$\mathrm{L}(\hat{\mathrm{y}}, \mathrm{y})=\sum_{\mathrm{t}=1}^{\mathrm{T}_{\mathrm{y}}} \mathrm{L}^{(\mathrm{t}>}\left(\hat{\mathrm{y}}^{(\mathrm{t}>}, \mathrm{y}^{(\mathrm{t}>}\right)$

然后，反向传播（Backpropagation）过程就是从右到左分别计算$\mathrm{L}(\hat{\mathrm{y}}, \mathrm{y})$对参数$W_a, W_y, b_a, b_y$的偏导数。思路与做法与标准的神经网络是一样的。这种从右到左的求导过程被称为 Backpropagation through time。

![](image/2022-07-29-18-56-56.png)

![](image/2022-07-29-18-57-03.png)

### 6.1.5 不同类型的RNN

根据 Tx 与 Ty 的关系，RNN 模型包含以下几个类型：

**many-to-many( $T_x = T_y$ )(命名实体识别):**


![](image/2022-07-27-10-23-58.png)

**many-to-one(Tx>1,Ty=1(情感分类))：**


![](image/2022-07-27-10-25-04.png)

**one-to-many(Tx=1,Ty>1(音乐生成))：**

![](image/2022-07-27-10-25-46.png)

**many-to-many(Tx≠Ty(机器翻译))：**

![](image/2022-07-27-10-28-14.png)


### 6.1.6 语言模型和序列生成

>语言模型：语言模型就是用来计算一个句子的概率的模型，也就是判断一句话是否是人话的概率。

语言模型是自然语言处理（NLP）中最基本和最重要的任务之一。使用 RNN 能够很好地建立需要的不同语言风格的语言模型。

**什么是语言模型？**

举例如下，在语音识别中，某句语音有两种翻译：

![](image/2022-07-27-10-29-00.png)

很明显，第二句话更有可能是正确的翻译。语言模型实际上会计算出这两句话各自的出现概率。比如第一句话概率为 $10^{-13}$，，第二句话概率为$10^{-10}$，概率计算的表达式为：

$\mathrm{P}\left(\mathrm{y}^{(1>}, \mathrm{y}^{(2>}, \cdots, \mathrm{y}^{(\mathrm{T}_{\mathrm{y}}>}\right)$

**使用RNN构建语言模型：**

首先，需要一个足够大的训练集，训练集由大量的单词语句语料库（corpus）构成。然后，对 corpus 的每句话进行切分词（tokenize）。做法就跟第 2 节介绍的一样，建立 vocabulary，对每个单词进行 one-hot 编码。

准备好训练集并对语料库进行切分词等处理之后，接下来构建相应的 RNN 模型。

![](image/2022-07-29-19-05-52.png)

语言模型的 RNN 结构如上图所示，$\mathrm{x}^{(1>}$和$\mathrm{a}^{(0>}$均为零向量。Softmax 输出层$\hat{y}^{(1>}$表示出现该语句第一个单词的概率，softmax 输出层 $\hat{y}^{(2>}$ 表示在第一个单词基础上出现第二个单词的概率，即条件概率，以此类推，最后是出现$(\mathrm{EOS}>$的条件概率.

单个元素的 softmax loss function 为：

$\mathrm{L}^{(\mathrm{t}>}\left(\hat{\mathrm{y}}^{(\mathrm{t}>}, \mathrm{y}^{(\mathrm{t}>}\right)=-\sum_{i} \mathrm{y}_{\mathrm{i}}^{(\mathrm{t}>} \log \hat{\mathrm{y}}_{\mathrm{i}}^{(\mathrm{t}>}$

该样本所有元素的 Loss function 为：

$\mathrm{L}(\hat{\mathrm{y}}, \mathrm{y})=\sum_{\mathrm{t}} \mathrm{L}^{(\mathrm{t}>}\left(\hat{\mathrm{y}}^{(\mathrm{t}>}, \mathrm{y}^{(\mathrm{t}>}\right)$

对语料库的每条语句进行 RNN 模型训练，最终得到的模型可以根据给出语句的前几个单词预测其余部分，将语句补充完整。例如给出 “Cats average 15”，RNN 模型可能预测完整的语句是 “Cats average 15 hours of sleep a day.”。

### 6.1.7 新序列采样

利用训练好的 RNN 语言模型，可以进行新的序列采样，从而随机产生新的语句。

- 首先输入 $x^{(1>} = 0,a^{(0>} = 0$，在这第一个时间步，得到所有可能的输出经过$softmax$层后可能的概率，根据这个$softmax$的分布，进行随机采样，获取第一个随机采样单词$\hat{y}^{(1>}$
- 然后继续下一个时间步，以刚刚采样得到的$\hat{y}^{(1>}$作为下一个时间步的输入，进而
$softmax$层会预测下一个输出$\hat{y}^{(2>}$，依次类推
- 如果字典中有结束的标志如：“EOS”，那么输出是该符号时则表示结束；若没有这种标志，则可以自行设置结束的时间步。

![](image/2022-07-27-11-51-45.png)

以上介绍的是 word level RNN，即每次生成单个 word，语句由多个 words 构成。另外一种情况是 character level RNN，即词汇表由单个英文字母或字符组成，如下所示：

![](image/2022-07-27-11-52-02.png)

- 优点：character level RNN 能有效避免遇到词汇表中不存在的单词$(\mathrm{UNK}>$
- 缺点：由于是字符表征，每句话的字符数量很大，这种大的跨度不利于寻找语句前部分和后部分之间的依赖性。另外，character level RNN 的在训练时的计算量也是庞大的。基于这些缺点，目前 character level RNN 的应用并不广泛，但是在特定应用下仍然有发展的趋势。

### 6.1.8 RNN的梯度消失

语句中可能存在跨度很大的依赖关系，即某个 word 可能与它距离较远的某个 word 具有强依赖关系。例如下面这两条语句：

- The cat, which already ate ...........，was full；
- The cats, which already ate ...........，were full.

第一句话中，was 受 cat 影响；第二句话中，were 受 cats 影响。它们之间都跨越了很多单词。而一般的 RNN 模型每个元素**受其周围附近的影响较大，难以建立跨度较大的依赖性**（long-term dependicies）。上面两句话的这种依赖关系，由于跨度很大，普通的 RNN 网络容易出现梯度消失，捕捉不到它们之间的依赖，造成语法错误。

另一方面，RNN 也可能出现梯度爆炸的问题，即 gradient 过大。常用的解决办法是设定一个阈值，一旦梯度最大值达到这个阈值，就对整个梯度向量进行尺度缩小。这种做法被称为 gradient clipping。

![](image/2022-07-27-11-53-08.png)

### 6.1.9 GRU单元

>GRU 的基本思想是通过加入门，扑捉非常长范围的依赖

RNN单元：

对于RNN的一个时间步的计算单元，在计算$a^{(t>}$也就是下图右边的公式，能以左图的形式可视化呈现：

![](image/2022-07-27-11-54-18.png)

**简化的GRU单元：**

以时间步从左到右进行计算的时候，在GRU单元中，存在一个新的变量称为  ，（代表cell）,作为“记忆细胞”，其提供了长期的记忆能力。

![](image/2022-07-27-11-54-48.png)

GRU的可视化实现如下图右边所示：

![](image/2022-07-27-11-55-08.png)

**完整的GRU 单元：**

完整的GRU单元还存在另外一个门，以决定每个时间步的候选值，公式如下：

![](image/2022-07-27-11-55-37.png)

### 6.1.10 LSTM

长短期记忆（long short-term memory, LSTM）对捕捉序列中更深层次的联系要比GRU更加有效。

LSTM中，使用了单独的更新门$\Gamma_u$和$\Gamma_f$，以及一个输出门$\Gamma_o$，主要公式如下：

![](image/2022-07-27-17-54-56.png)

LSTM单元的可视化图如下所示：

![](image/2022-07-27-17-55-14.png)

### 6.1.11 双向RNN

双向RNN（bidirectional RNNs）模型能够让我们在序列的某处，不仅可以获取之间的信息，还可以获取未来的信息。

对于下图的单向RNN的例子中，无论RNN单元是基本的RNN单元，还是GRU，或者LSTM单元，对于例子中第三个单词"Teddy"很难判断是否是人名，仅仅使用前面的两个单词是不够的，需要后面的信息来进行判断，但是单向RNN就无法实现获取未来的信息。

![](image/2022-07-27-17-55-55.png)

而双向RNN则可以解决单向RNN存在的弊端。在BRNN中，不仅有从左向右的前向连接层，还存在一个从右向左的反向连接层。

![](image/2022-07-27-17-56-12.png)

在NLP问题中，常用的就是使用双向RNN的LSTM。

### 6.1.12 深层RNNs

与深层的基本神经网络结构相似，深层RNNs模型具有多层的循环结构，但不同的是，在传统的神经网络中，可能会拥有很多层，几十层上百层，但是对与RNN来说，三层的网络结构就已经很多了，因为RNN存在时间的维度，所以其结构已经足够的庞大。如下图所示：

![](image/2022-07-27-17-56-39.png)

## 6.2 自然语言处理和词嵌入

### 6.2.1 词汇表征

为了让计算机能够能更好地理解人类的语言，建立更好的语言模型，需要将词汇进行表征。下面是几种不同的词汇表征方式：

**one-hot 表征：**

对应单词的位置用1表示，其余位置用0表示，如下图所示：

![](image/2022-07-28-10-27-30.png)

one-hot表征的缺点：这种方法将每个词孤立起来，使得模型对相关词的泛化能力不强。每个词向量之间的距离都一样，乘积均为0，所以无法获取词与词之间的相似性和关联性。

**特征表征：词嵌入**

用不同的特征来对各个词汇进行表征，相对与不同的特征，不同的单词均有不同的值。如下例所示：

![](image/2022-07-28-10-28-08.png)

这种表征方式使得词与词之间的相似性很容易地表征出来，这样对于不同的单词，模型的泛化性能会好很多。下面是使用t-SNE算法将高维的词向量映射到2维空间，进而对词向量进行可视化，很明显可以看出对于相似的词总是聚集在一块儿：

![](image/2022-07-28-10-28-27.png)

### 6.2.2 使用 Word Embeddings

Word Embeddings对不同单词进行了实现了特征化的表示。

**名字实体识别的例子：**

如下面的一个句子中名字实体的定位识别问题，假如有一个比较小的数据集，可能不包含durain（榴莲）和cultivator（培育家）这样的词汇，那么就很难从包含这两个词汇的句子中识别名字实体。但是如果可以从网上的其他地方获取了一个学习好的word Embedding，它将告诉我们榴莲是一种水果，并且培育家和农民相似，那么就有可能从少量的训练集中，归纳出没有见过的词汇中的名字实体。

![](image/2022-07-28-10-29-41.png)

**词嵌入的迁移学习：**

词嵌入迁移学习的步骤：
- 第一步：从大量的文本集合中学习word Embeddings（1-100B words），或者从网上下载预训练好的词嵌入模型；
- 第二步：将词嵌入模型迁移到本地小训练集的新任务上；
- 第三步：可选，使用新的标记数据对词嵌入模型继续进行微调。

**词嵌入和人脸编码：**

词嵌入和人脸编码之间有很奇妙的联系。在人脸识别领域，通常会将人脸图片预编码成不同的编码向量，以表示不同的人脸，进而在识别的过程中使用编码来进行比对识别。词嵌入则和人脸编码有一定的相似性。

![](image/2022-07-28-10-30-55.png)

但是不同的是，对于人脸识别，可以将任意一个没有见过的人脸照片输入到本地构建的网络中，则可输出一个对应的人脸编码。而在词嵌入模型中，所有词汇的编码是在一个固定的词汇表中进行学习单词的编码以及其之间的关系的。

### 6.2.3 词嵌入的特性

**类比推理特性：**

词嵌入还有一个重要的特性，它还能够帮助实现类比推理。如下面的例子中，通过不同词向量之间的相减计算，可以发现不同词之间的类比关系，man——woman、king——queen，如下图所示：

![](image/2022-07-28-10-31-59.png)

计算词与词之间的相似度，实际上是在多维空间中，寻找词向量之间各个维度的距离相似度。

![](image/2022-07-28-10-32-28.png)

**相似度函数：**

![](image/2022-07-28-10-33-01.png)

### 6.2.4  嵌入矩阵

在对一个词汇表学习词嵌入模型时，实质上就是要学习这个词汇表对应的一个嵌入矩阵$E$。当学习好了这样一个嵌入矩阵后，通过嵌入矩阵与对应词的one-hot向量相乘，则可得到该词汇的embedding，如下图所示：

![](image/2022-07-28-10-34-01.png)

### 6.2.5 学习词嵌入

**上下文和目标词对：**

将要预测的单词称为目标词，其是通过一些上下文推导预测出来的。对于不同的问题，上下文的大小和长度以及选择的方法有所不同。

- 选取目标词之前的几个词；
- 选取目标词前后的几个词；
- 选取目标词前的一个词；
- 选取目标词附近的一个词，（一种Skip-Gram模型的思想）。

### 6.2.6 Word2Vec

Word2Vec算法是一种简单的计算更加高效的方式来实现对词嵌入的学习。

**Skip-grams：**

在Skip-grams模型中，需要抽取上下文（Content）和目标词（Target）配对，来构造一个监督学习问题。

上下文不一定是要目标词前面或者后面离得最近的几个单词，而是随机选择一个词作为上下文，同时在上下文的一定距离范围内随机选择另外一个词作为目标词。构造这样一个监督学习问题的目的，并不是想要解决监督学习问题本身，而是想要使用这个问题来学习一个好的词嵌入模型。

**模型流程：**

![](image/2022-07-28-10-36-31.png)

**存在的问题：**

- 计算速度的问题，如在上面的 $Softmax$单元中，需要对所有10000个整个词汇表的词做求和计算，计算量庞大。
- 简化方案：使用分级$Softmax$分类器，其计算复杂度是前面的$log|v|$级别。在构造分级$softmax$分类器时，一般常用的词会放在树的顶部位置，而不常用的词则会放在树的更深处，其并不是一个平衡的二叉树。

**如何采样上下文：**

在构建上下文目标词对时，如何选择上下文与模型有不同的影响。

- 对语料库均匀且随机地采样：使得如the、of、a等这样的一些词会出现的相当频繁，导致上下文和目标词对经常出现这类词汇，但想要的目标词却很少出现。
- 采用不同的启发来平衡常见和不常见的词进行采样。这种方法是实际使用的方法。

### 6.2.7 负采样

**新的学习问题：**

- 定义一个新的学习问题：预测两个词之间是否是上下文-目标词对，如果是词对，则学习的目标为1；否则为0。
- 使用k次相同的上下文，随机选择不同的目标词，并对相应的词对进行正负样本的标记，生成训练集。
- 建议：小数据集，k=5 ~ 20；大数据集，k=2 ~ 5。
- 最后学习$x-y$的映射关系。

![](image/2022-07-28-10-43-38.png)

**模型：**

在负采样模型中，可以使用logistic回归模型：

$P(y=1|c,t) = \sigma(\theta_t^Te_c)$

每个正样本均有k个对应的负样本。在训练的过程中，对于每个上下文词，就有对应的$k+1$个分类器。如下图所示：

![](image/2022-07-28-10-54-03.png)

相比与Skip-grams模型，负采样不再使用一个具有词汇表大小时间复杂度高的庞大维度的Softmax，而是将其转换为词汇表大小个二分类问题。每个二分类任务都很容易解决，因为每个的训练样本均是1个正样本，外加k个负样本。

**如何选择负样本：**

在选定了上下文（Content）后，在确定正样本的情况下，还需要选择k个负样本以训练每个上下文的分类器。

![](image/2022-07-28-10-54-48.png)

### 6.2.8 GloVe 词向量

**GloVe模型：**

![](image/2022-07-28-10-59-36.png)

从上面的目标中，可以得出想要学习一些向量，他们的输出能够对上下文和目标两个词同时出现的频率进行很好的预测，从而得到想要的词嵌入向量。

**词嵌入的特征化：**

通过上面的很多算法得到的词嵌入向量，无法保证词嵌入向量的每个独立分量是能够可以理解的。能够确定是每个分量是和预想的一些特征是有关联的，其可能是一些能够理解的特征的组合而构成的一个组合分量。使用上面的GloVe模型，从线性代数的角度解释如下：

![](image/2022-07-28-11-00-49.png)

加入的$A$项，可能构成任意的分量组合。

![](image/2022-07-28-11-01-11.png)

### 6.2.9 情感分类

情感分类就是通过一段文本来判断这个文本中的内容是否喜欢其所讨论的内容，这是NLP中最重要的模块之一。

![](image/2022-07-28-11-01-26.png)

情感分类任务存在的一个问题就是只有很小的数据集，缺乏训练样本。但是在使用了词嵌入后，则能够带来很好的效果，足以训练一个良好的情感分类模型。

**平均值或和的模型：**

- 获取一个训练好的词嵌入矩阵 $E$ 
- 得到每个词的词嵌入向量，并对所有的词向量做平均或者求和
- 输入到$softmax$分类器中，得到最后的输出$\hat{y}$
- 缺点：没有考虑词序，可能会导致多数的积极词汇削弱前面消极词汇的影响，从而造成错误的预测。

![](image/2022-07-28-11-03-08.png)

**RNN模型：**

- 取一个训练好的词嵌入矩阵$E$
- 得到每个词的词嵌入向量，输入到many-to-one的RNN模型中
- 通过最后的softmax分类器，得到最后的输出$\hat{y}$
- 优点：考虑了词序，效果好很多

![](image/2022-07-28-11-06-19.png)

### 6.2.10 词嵌入消除偏见

**目前的偏见问题：**

以一些预料库中学习到的词嵌入向量，会发现学习到的词向量存在下面一些具有性别、种族等偏见，这反映了人们在历史的写作中存在的这种社会偏见：

![](image/2022-07-28-11-07-46.png)

**消除偏见的方法：**

- 定义偏见的方向：如性别
- 对大量性别相对的词汇进行相减并求平均
- 通过平均后的向量，则可以得到一个或多个偏见趋势相关的维度，以及大量不相关的维度
- 中和化：对每一个定义不明确的词汇，进行偏见的处理，如像 doctor、babysitter 这类词；通过减小这些词汇在得到的偏见趋势维度上值的大小
- 均衡：将如 grandmother 和 grandfather 这种对称词对调整至babysitter这类词汇平衡的位置上，使babysitter这类词汇处于一个中立的位置，进而消除偏见

![](image/2022-07-28-11-08-49.png)



